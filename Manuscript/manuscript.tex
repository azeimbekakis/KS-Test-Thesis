\documentclass[12pt, letterpaper, titlepage]{article}

\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{colorlinks = true, linkcolor = blue, citecolor=blue, urlcolor = blue}
\usepackage{natbib}
\usepackage{enumitem}
\usepackage{setspace}

\usepackage[pagewise]{lineno}
\linenumbers*[1]
% %% patches to make lineno work better with amsmath
\newcommand*\patchAmsMathEnvironmentForLineno[1]{%
 \expandafter\let\csname old#1\expandafter\endcsname\csname #1\endcsname
 \expandafter\let\csname oldend#1\expandafter\endcsname\csname end#1\endcsname
 \renewenvironment{#1}%
 {\linenomath\csname old#1\endcsname}%
 {\csname oldend#1\endcsname\endlinenomath}}%
\newcommand*\patchBothAmsMathEnvironmentsForLineno[1]{%
 \patchAmsMathEnvironmentForLineno{#1}%
 \patchAmsMathEnvironmentForLineno{#1*}}%

\AtBeginDocument{%
 \patchBothAmsMathEnvironmentsForLineno{equation}%
 \patchBothAmsMathEnvironmentsForLineno{align}%
 \patchBothAmsMathEnvironmentsForLineno{flalign}%
 \patchBothAmsMathEnvironmentsForLineno{alignat}%
 \patchBothAmsMathEnvironmentsForLineno{gather}%
 \patchBothAmsMathEnvironmentsForLineno{multline}%
}



\title{Survey of Misuses of the Kolmogorov–Smirnov-Test}

\author{Anthony Zeimbekakis and Jun Yan\\
\href{mailto:anthony.zeimbekakis@uconn.edu}{\nolinkurl{anthony.zeimbekakis@uconn.edu}}\\
Department of Statistics, University of Connecticut}
\date{January 23, 2021}

\begin{document}
\maketitle

\doublespace

\begin{abstract}
The Kolmogorov–Smirnov (KS) test is one the most popular goodness-of-fit tests for comparing a sample with a hypothesized parametric distribution. Nevertheless, it has often been misused. The standard one-sample KS test applies to independent, continuous data with a hypothesized distribution that is completely specified. It is not uncommon, however, to see in the literature that it was applied to dependent, discrete, or rounded data, with hypothesized distributions containing estimated parameters. For example, it has been “discovered” multiple times that the test is too conservative when the parameters are estimated [e.g. \citet{Steinskog}]. This paper aims to survey the misuses of the KS test, demonstrate their consequences through simulation, and provide remedies as needed.
\end{abstract}


\hypertarget{sec:intro}{%
\section{Introduction}\label{sec:intro}}

The Kolmogorov-Smirnov (K-S) statistic is one of the most popular goodness-of-fit tests for comparing a sample with a hypothesized parametric distribution. Given a sample of n observations, let $S_{n}(x)$ be the empirical cumulative distribution and $F(x)$ be the population cumulative distribution. The K-S statistic is defined by: \[d = max(\lvert F(x)-S_{n}(x) \rvert)\] If the value of $d$ exceeds the critical value given by the table, the null hypothesis that the observations are from a specified distribution is rejected. 

However, the test is often misused. The standard one-sample K-S test applies to independent, continuous data with a hypothesized distribution that is completely satisfied. Often in literature, the test is applied to dependent, discrete, or rounded data, with hypothesized distributions containing estimated parameters. As shown by \citet{Steinskog} and later in this paper, the test is too conservative when the parameters are estimated. Throughout this paper, the cumulative distribution $F(x)$ is standard normal.

To begin, a simple demonstration is performed to show the impact of sample size and replicate tests. For $n$ in $[10, 100, 1000]$, a random standard normal distributions were generated and K-S tests were performed. With the hypothesized distribution of standard normal, the histograms in Figure 1 show that the sample size has little effect on the distribution of P-values.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.8\linewidth]{hist_normal}
  \caption{Histograms of impact of sample size and number of tests on P-value.}
  \label{fig:hist_normal}
\end{figure}

\hypertarget{sec:fitted}{%
\section{Fitted Parameters}\label{sec:fitted}}

The K-S test shows significant issues in the case of fitted parameters.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.8\linewidth]{hist_parametric}
  \caption{Histograms of P-values with parametric bootstrap.}
  \label{fig:hist_parametric}
\end{figure}

\hypertarget{sec:rounded}{%
\section{Rounded Data}\label{sec:rounded}}

Another assumption of the K-S test is that it applies to continuous data. Of course, in any case where the distribution is discrete, the test no longer applies and loses its power. To demonstrate this, simulations were conducted at varying levels of rounding. A random sample of size $10,000$ was collected from the standard normal distribution and subsequently rounded to the nearest degree, up to the ten thousandth. This simulation was replicated $10,000$ times for each rounding level and the results are shown in Figure~\ref{fig:hist_rounded}.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.8\linewidth]{hist_rounded}
  \caption{Histograms of P-values with rounded data.}
  \label{fig:hist_rounded}
\end{figure}

From the histograms it is clear that though it appears the distributions of p-values hold a uniform shape through rounding to the thousandth, it quickly deviates as the rounding becomes stronger. This simulation shows that rounding can have an impact on the power of the K-S test as early as the hundreths place and discrete data should not be used.

There are some solutions in the case that a discrete distribution is in question. The chi-squared test [INSERT REFERENCE HERE] is another goodness-of-fit test that can be applied to discrete distributions. However, the standard K-S test holds various advantages over it. In small samples, \citet{Massey} explains that the chi-squared test loses information and validity due to the grouping performed, while the K-S test treats individual observations separately. \citet{Massey} additionally notes that the K-S test generally appears to be the more powerful test regardless of sample size.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.8\linewidth]{hist_dgof}
  \caption{Histograms of P-values with dgof package.}
  \label{fig:hist_dgof}
\end{figure}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.8\linewidth]{hist_dgofsim}
  \caption{Histograms of P-values when simulating p-value.}
  \label{fig:hist_dgofsim}
\end{figure}

\hypertarget{sec:correlation}{%
\section{Problem under Serial Dependence}\label{sec:correlation}}

The K-S test shows significant issues in the case of serial dependence.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.8\linewidth]{hist_correlation}
  \caption{Histograms of P-values with correlated data.}
  \label{fig:hist_correlation}
\end{figure}

\hypertarget{sec:conclusion}{%
\section{Conclusion}\label{sec:conclusion}}

Conclusion here.

Adding these to see the full bibliography: 

\citet{Steinskog}
\citet{Weiss}
\citet{Massey}
\citet{Lilliefors}
\citet{dgof}
\citet{Conover}
\citet{Gleser}

\bibliographystyle{chicago}
\bibliography{citations.bib}


\end{document}
